function [ VectorAccuracy, VectorMeanAcc ] = ClusterSVMQP_scan( subset_unbiased, sublabel_unbiased, clusterlabelset, K_cv, k, logC )
%ClusterSVMQP_scan
%%function
% invoking ClusterSVMQP_crossvalind, ClusterSVMQP_train, ClusterSVMQP_test repeatly.
% This function is to find out the best penalty efficient using normal
% crossvalind

%%input
% subset_unbiased: n*m matrix which comprises balanced two-class susbets
% sublabel_unbiased: corresponding labelsets
% clusterlabelset: indicates which label the points belong to
% K_cv: K-fold-crossvalind. for the details about K-Fold-CrossValind£¬pls google it
% k: num of clusters
% logC: penalty coeff. for the details about penalty coeff, pls google "Soft SVM".

%%ouput
% VectorAccuracy: the upper of test accuracy, is generated by using the
% whole dataset as both training and testing set.

% VectorMeanAcc: the true test accuracy.


    C = 2.^logC;
    nC = length(logC);
    VectorAccuracy = zeros(1,nC);
    VectorMeanAcc = zeros(1,nC);
    VectorStdAcc = zeros(1,nC);

    for i = 1:nC
        [ w ,b ] = ClusterSVMQP_train( subset_unbiased, sublabel_unbiased, clusterlabelset, k, C(i) );
        [ VectorAccuracy(i) ] = ClusterSVMQP_test( subset_unbiased, sublabel_unbiased, clusterlabelset, w, b );
        [ VectorMeanAcc(i), VectorStdAcc(i) ] = ClusterSVMQP_crossvalind( subset_unbiased, sublabel_unbiased, clusterlabelset, K_cv, k, C(i) );
        i
    end 
    
    figure;
    subplot(211)
    plot(logC,[VectorAccuracy;VectorMeanAcc]);
    subplot(212)
    plot(logC,VectorStdAcc);

end

