function [ VectorAccuracy, VectorMeanAcc ] = qssvm_scan( subset_unbiased, sublabel_unbiased, logC, K )
%qssvm_scan
%%function
% invoking qssvm_crossvalind, qssvm_train, qssvm_test repeatly.
% This function is to find out the best penalty efficient using normal
% crossvalind

%%input
% subset_unbiased: n*m matrix which comprises balanced two-class susbets
% sublabel_unbiased: corresponding labelsets
% K: K-fold-crossvalind. for the details about K-Fold-CrossValind£¬pls google it
% logC: penalty coeff. for the details about penalty coeff, pls google "Soft SVM".

%%ouput
% VectorAccuracy: the upper of test accuracy, is generated by using the
% whole dataset as both training and testing set.

% VectorMeanAcc: the true test accuracy.

    if(nargin <3)
        logC = 1:12;
        K = 10;
    end

    C = 2.^logC;
    nC = length(logC);
    VectorAccuracy = zeros(1,nC);
    VectorMeanAcc = zeros(1,nC);
    VectorStdAcc = zeros(1,nC);

    for i = 1:nC
        [ W, b, c ] = qssvm_train( subset_unbiased , sublabel_unbiased , C(i));
        [ ~, ~, ~ ,~, VectorAccuracy(i) ] = qssvm_test( subset_unbiased, sublabel_unbiased, W, b, c  );
        [ VectorMeanAcc(i), VectorStdAcc(i) ] = qssvm_crossvalind( subset_unbiased, sublabel_unbiased, K, C(i) );
        i
    end 
    
    figure;
    subplot(211)
    plot(logC,[VectorAccuracy;VectorMeanAcc]);
    subplot(212)
    plot(logC,VectorStdAcc);

end

